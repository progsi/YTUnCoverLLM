{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 8886.24it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.49it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).cuda()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=local_llm\n",
    "                     )\n",
    "\n",
    "question = \"What is the capital of England?\"\n",
    "\n",
    "print(llm_chain.run(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroShot with Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not infer framework from class <class 'langchain_community.chat_models.ollama.ChatOllama'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere is the source text: \u001b[39m\u001b[38;5;132;01m{source_text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m template \u001b[38;5;241m=\u001b[39m instruction \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m---> 27\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m local_llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n\u001b[1;32m     37\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     38\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     39\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/transformers/pipelines/base.py:301\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/transformers/utils/generic.py:753\u001b[0m, in \u001b[0;36minfer_framework\u001b[0;34m(model_class)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not infer framework from class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not infer framework from class <class 'langchain_community.chat_models.ollama.ChatOllama'>."
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# Define the prompt template with detailed instructions\n",
    "instruction = \"\"\"\n",
    "You are an information extraction model which extracts attributes given an input text.\n",
    "Extract the following music attributes from the given Reddit post:\n",
    "- Work of Art (WoA): The title of the song or album mentioned in the text.\n",
    "- Performer: Performer(s) of the song or album mentioned in the text.\n",
    "- Additional Performers: Performers who are not explicitly mentioned in the source text but are relevant.\n",
    "- Title Contextual Cue: Text from the source that indicates the song or album title (e.g. \"the song\" in the phrase \"the song X\").\n",
    "- Performer Contextual Cue: Text from the source that indicates the performer(s) (e.g. \"is a song by\" in the phrase \"is a song by A\").\n",
    "\n",
    "Provide a structured output in JSON format with the following keys:\n",
    "- title: (string) representing the WoA or song titles or album titles mentioned in the text.\n",
    "- performer: (string) performer(s) of the song or album mentioned in the text.\n",
    "- performer_unmentioned: (string) additional performers not contained in the source text.\n",
    "- title_cue: (string) text from the source indicating the song title/album title.\n",
    "- performer_cue: (string) text from the source indicating the performer.\n",
    "\n",
    "Your output should be a JSON object structured as described above.\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"Here is the source text: {source_text}\"\n",
    "template = instruction + suffix\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"source_text\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=local_llm, prompt=prompt)\n",
    "\n",
    "# Example usage\n",
    "source_text = \"Check out Blinding Lights by The Weeknd. It is so good!\"\n",
    "result = llm_chain.run({\"source_text\": source_text})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend before going to Mars?\n",
      "\n",
      "Because he needed space! (get it?)\n"
     ]
    }
   ],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/concepts/#langchain-expression-language-lcel\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "with open(\"../keys/openai.txt\", \"r\") as f:\n",
    "    key = f.read()\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = key\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Define your desired data structure.\n",
    "class WorkOfArt(BaseModel):\n",
    "    title: str = Field(description=\"The title of the song or album mentioned in the text.\")\n",
    "    title_cue: str = Field(description=\"Text from the source indicating the song title/album title\")\n",
    "    performer: str = Field(description=\"Performer(s) of the song or album mentioned in the text.\")\n",
    "    performer_unmentioned: str = Field(description=\"Performers who are not explicitly mentioned in the source text but are relevant.\")\n",
    "    performer_cue: str = Field(description=\"Text from the source indicating the performer.\")\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=WorkOfArt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorkOfArt(title='Blinding Lights', title_cue='by The Weeknd. It is so good!', performer='The Weeknd', performer_unmentioned='', performer_cue='')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Extract the following music attributes from the given Reddit post.\n",
    "    Here are the formatting instructions:{format_instructions}\n",
    "    Here is the source text:\\n{source_text}\"\"\",\n",
    "    input_variables=[\"source_text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "result = chain.invoke({\"source_text\": source_text})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.WorkOfArt'>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/torch21/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m chain({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_text})\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Parse the output using the parser\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:72\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m])\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=local_llm, prompt=prompt)\n",
    "generated_output = chain({\"source_text\": source_text})\n",
    "\n",
    "# Parse the output using the parser\n",
    "result = parser.parse(generated_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'title': {'title': 'Title',\n",
       "   'description': 'The title of the song or album mentioned in the text.',\n",
       "   'type': 'string'},\n",
       "  'title_cue': {'title': 'Title Cue',\n",
       "   'description': 'Text from the source indicating the song title/album title',\n",
       "   'type': 'string'},\n",
       "  'performer': {'title': 'Performer',\n",
       "   'description': 'Performer(s) of the song or album mentioned in the text.',\n",
       "   'type': 'string'},\n",
       "  'performer_unmentioned': {'title': 'Performer Unmentioned',\n",
       "   'description': 'Performers who are not explicitly mentioned in the source text but are relevant.',\n",
       "   'type': 'string'},\n",
       "  'performer_cue': {'title': 'Performer Cue',\n",
       "   'description': 'Text from the source indicating the performer.',\n",
       "   'type': 'string'}},\n",
       " 'required': ['title',\n",
       "  'title_cue',\n",
       "  'performer',\n",
       "  'performer_unmentioned',\n",
       "  'performer_cue']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the following music attributes from the given Reddit post. Here is the source text:\n",
      "\u001b[33;1m\u001b[1;3m{source_text}\u001b[0m\n",
      "Here are the formatting instructions:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"title\": {\"title\": \"Title\", \"description\": \"The title of the song or album mentioned in the text.\", \"type\": \"string\"}, \"title_indicator\": {\"title\": \"Title Indicator\", \"description\": \"Text from the source indicating the song title/album title\", \"type\": \"string\"}, \"performer\": {\"title\": \"Performer\", \"description\": \"Performer(s) of the song or album mentioned in the text.\", \"type\": \"string\"}, \"performer_unmentioned\": {\"title\": \"Performer Unmentioned\", \"description\": \"Performers who are not explicitly mentioned in the source text but are relevant.\", \"type\": \"string\"}, \"performer_indicator\": {\"title\": \"Performer Indicator\", \"description\": \"Text from the source indicating the performer.\", \"type\": \"string\"}}, \"required\": [\"title\", \"title_indicator\", \"performer\", \"performer_unmentioned\", \"performer_indicator\"]}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Few-shot examples (optional)\n",
    "examples = [\n",
    "{\n",
    "    \"source_text\": \"I just listened to Shape of You by Ed Sheeran. It's amazing!\",\n",
    "    \"title\": \"Shape of You\",\n",
    "    \"performer\": \"Ed Sheeran\",\n",
    "    \"performer_unmentioned\": \"\",\n",
    "    \"title_cue\": \"listened to\",\n",
    "    \"performer_cue\": \"by\"\n",
    "},\n",
    "{\n",
    "    \"source_text\": \"The album 'Abbey Road' by The Beatles is a classic.\",\n",
    "    \"title\": \"Abbey Road\",\n",
    "    \"performer\": \"The Beatles\",\n",
    "    \"performer_unmentioned\": \"\",\n",
    "    \"title_cue\": \"The album\",\n",
    "    \"performer_cue\": \"by\"\n",
    "}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"source_text\", \"title\", \n",
    "                     \"performer\", \"performer_unmentioned\",\n",
    "                     \"title_indicator\", \"performer_indicator\"], \n",
    "                     template=\"\"\"Source text: {source_text}; Output: \n",
    "                            'title': {title}, 'performer': {performer}, \n",
    "                            'performer_unmentioned': {performer_unmentioned},\n",
    "                            'title_indicator': {title_indicator},\n",
    "                            'performer_indicator': {performer_indicator}\"\n",
    "                            \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "fewshot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=instruction + \"\\nHere are some examples: \",\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"source_text\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the following music attributes from the given Reddit post. Here is the source text:\n",
      "\u001b[33;1m\u001b[1;3m{source_text}\u001b[0m\n",
      "Here are the formatting instructions:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"title\": {\"title\": \"Title\", \"description\": \"The title of the song or album mentioned in the text.\", \"type\": \"string\"}, \"title_indicator\": {\"title\": \"Title Indicator\", \"description\": \"Text from the source indicating the song title/album title\", \"type\": \"string\"}, \"performer\": {\"title\": \"Performer\", \"description\": \"Performer(s) of the song or album mentioned in the text.\", \"type\": \"string\"}, \"performer_unmentioned\": {\"title\": \"Performer Unmentioned\", \"description\": \"Performers who are not explicitly mentioned in the source text but are relevant.\", \"type\": \"string\"}, \"performer_indicator\": {\"title\": \"Performer Indicator\", \"description\": \"Text from the source indicating the performer.\", \"type\": \"string\"}}, \"required\": [\"title\", \"title_indicator\", \"performer\", \"performer_unmentioned\", \"performer_indicator\"]}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"../keys/huggingface.txt\", \"r\") as f:\n",
    "    api_token = f.read()\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = api_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    model_kwargs={\"temperature\":0, \"max_length\":180}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=HuggingFaceHub(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                                        model_kwargs={\"temperature\":0.001, \n",
    "                                                      \"max_length\":64}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "\n",
      "Answer: Let's think step by step. France is a country located in Western Europe. The capital of France is... Paris! That's right! The City of Light, famous for its iconic landmarks like the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. Voil√†! üá´üá∑üëç\n",
      "#### 1.5/1.5 points\n",
      "#### 100% accuracy\n",
      "#### 1.5/1.5 points\n",
      "#### 100% accuracy\n",
      "#### 1\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "\n",
    "print(llm_chain.run(question))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

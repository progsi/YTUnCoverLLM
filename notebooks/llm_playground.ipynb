{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/torch21/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/miniconda3/envs/torch21/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6697.49it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.02s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.Wrapper import LlamaWrapper\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama = LlamaWrapper(model_id=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = [\n",
    "    \"Dream Theater Metropolis Pt. 1 (Live At Luna Park DVD)\", \n",
    "    \"Plini & David Maxim Micic LIVE @ Vh1 Supersonic 2018 (full set)\",\n",
    "    \"(Måneskin) Beggin' - Fingerstyle Guitar Cover | Josephine Alexandra\",\n",
    "    \"Owane - Rock Is Too Heavy\",\n",
    "    \"Jungle - Tash Sultana - Tutorial - Guitar Loop Cover - Tabs Available\",\n",
    "    \"How to Play Beat It Solo - Eddie Van Halen Michael Jackson\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artist and Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1) manually with full list --> unstructured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Utils import read_jsonfile\n",
    "\n",
    "roles = read_jsonfile(\"../prompts/roles.json\")\n",
    "tasks = read_jsonfile(\"../prompts/tasks.json\")\n",
    "examples = read_jsonfile(\"../prompts/examples.json\")\n",
    "schemas = read_jsonfile(\"../prompts/schemas.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m      7\u001b[0m top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m output\n",
      "File \u001b[0;32m/data/repos/YTUnCoverLLM/src/Wrapper.py:52\u001b[0m, in \u001b[0;36mLlamaWrapper.prompt\u001b[0;34m(self, system_prompt, user_prompts, temperature, top_p, top_k)\u001b[0m\n\u001b[1;32m     49\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [system_prompt \u001b[38;5;241m+\u001b[39m user_prompt \u001b[38;5;28;01mfor\u001b[39;00m user_prompt \u001b[38;5;129;01min\u001b[39;00m user_prompts]\n\u001b[1;32m     51\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#outputs = self.tokenizer.batch_decode(generated_ids[:, model_inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/transformers/generation/utils.py:2897\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2896\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2897\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2899\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "role_description = roles[\"linguist_music\"]\n",
    "task_description = tasks[\"simple\"]\n",
    "\n",
    "prompt = role_description + task_description\n",
    "\n",
    "temperature = 0.01\n",
    "top_p = 0.1\n",
    "\n",
    "output = llama.prompt(prompt, title_list, temperature=temperature, top_p=top_p)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2) with jsonformer --> structured but only single json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With list arrays and string index. -> BAD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_description = roles[\"linguist_music\"]\n",
    "task_description = tasks[\"perf_position\"]\n",
    "json_schema = schemas[\"perf_position\"]\n",
    "\n",
    "outputs = []\n",
    "for title in title_list: \n",
    "    prompt = role_description + task_description + f\"'{title}'\"\n",
    "    outputs.append(llama.prompt_to_json(prompt, json_schema, 0.1))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple, few-shot -> okish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_description = roles[\"linguist_music\"]\n",
    "task_description = tasks[\"simple\"]\n",
    "json_schema = schemas[\"simple\"]\n",
    "\n",
    "outputs = []\n",
    "for title in title_list: \n",
    "    prompt = role_description + task_description + f\"'{title}'\"\n",
    "    outputs.append(llama.prompt_to_json(prompt, json_schema, 0.1))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple + 4 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'artist': 'Dream Theater', 'title': 'Metropolis Pt. 1'},\n",
       " {'artist': 'Plini;David Maxim Micic', 'title': 'full set'},\n",
       " {'artist': 'Måneskin;Josephine Alexandra', 'title': 'Beggin'},\n",
       " {'artist': 'Owane', 'title': 'Rock Is Too Heavy'},\n",
       " {'artist': 'Jungle;Tash Sultana', 'title': 'Tutorial - Guitar Loop Cover'},\n",
       " {'artist': 'Eddie Van Halen;Michael Jackson', 'title': 'Beat It'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_description = roles[\"linguist_music\"]\n",
    "task_description = tasks[\"simple\"]\n",
    "json_schema = schemas[\"simple\"]\n",
    "examples = examples[\"simple\"]\n",
    "\n",
    "outputs = []\n",
    "for title in title_list: \n",
    "    prompt = role_description + task_description + examples + f\"'{title}'\"\n",
    "    outputs.append(llama.prompt_to_json(prompt, json_schema, 0.1))\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_description = roles[\"linguist_music\"]\n",
    "task_description = tasks[\"translate_perf\"]\n",
    "json_schema = schemas[\"translate_perf\"]\n",
    "\n",
    "outputs = []\n",
    "for title in title_list: \n",
    "    task_description = f\"You are a given a video title of an online video which refers to a song. It is likely a cover. Parse the contained fields artist (covering and original), title (covering and original) and translate the title into english if its not english, else copy it into the translation. The title is: \"\n",
    "    prompt = role_description + task_description + f\"'{title}'\"\n",
    "    outputs.append(llama.prompt_to_json(prompt, json_schema, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_description = roles[\"linguist_music\"]\n",
    "task_description = tasks[\"simple\"]\n",
    "json_schema = schemas[\"simple\"]\n",
    "\n",
    "outputs = []\n",
    "for title in title_list: \n",
    "    task_description = f\"You are a given a video title of an online video which refers to a song. It is likely a cover. Parse the contained fields artist (covering and original), title (covering and original) and translate the title into english if its not english, else copy it into the translation. The title is: \"\n",
    "    prompt = role_description + task_description + f\"'{title}'\"\n",
    "    outputs.append(llama.prompt_to_json(prompt, json_schema, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"../data/shs100k2_processed.parquet\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHS100K-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet(\"/data/csi_datasets/shs100k2_yt.parquet\").query(\"split == 'TEST'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"artist_performing\": {\"type\": \"string\"},\n",
    "        \"artist_original\": {\"type\": \"string\"},\n",
    "        \"title_original\": {\"type\": \"string\"},\n",
    "        \"title_performing\": {\"type\": \"string\"},\n",
    "        \"title_english\": {\"type\": \"string\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "# construct prompts\n",
    "role_description = \"You are a linguistic and music expert.\"\n",
    "task_description = f\"You are a given a video title of an online video which refers to a song. It is likely a cover. Parse the contained fields artist (covering and original), title (covering and original) and translate the title into english if its not english, else copy it into the translation. The title is: \"\n",
    "prompts = [role_description + task_description + f\"'{title}'\" for title in data.video_title]\n",
    "\n",
    "# why can't I set to 0?\n",
    "temperature = 0.0001\n",
    "\n",
    "# inference with Llama\n",
    "outputs = []\n",
    "for prompt in tqdm(prompts): \n",
    "    output = llama.prompt_to_json(prompt, json_schema, temperature)\n",
    "    outputs.append(output)\n",
    "\n",
    "# write to df\n",
    "data_parsed = pd.DataFrame(outputs)\n",
    "for col in data_parsed.columns:\n",
    "    data[col] = data_parsed[col].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_title_perf = sum((data.title.str.lower() == data.title_performing.str.lower())) / len(data)\n",
    "accuracy_title_orig = sum((data.title.str.lower() == data.title_original.str.lower())) / len(data)\n",
    "accuracy_artist_perf = sum((data.performer.str.lower() == data.artist_performing.str.lower())) / len(data)\n",
    "accuracy_artist_orig = sum((data.performer.str.lower() == data.artist_original.str.lower())) / len(data)\n",
    "\n",
    "print(accuracy_title_perf)\n",
    "print(accuracy_title_orig)\n",
    "print(accuracy_artist_perf)\n",
    "print(accuracy_artist_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_perf_extracted = data.apply(lambda x: x.title_performing in x.video_title, axis=1).sum() / len(data)\n",
    "title_orig_extracted = data.apply(lambda x: x.title_original in x.video_title, axis=1).sum() / len(data)\n",
    "artist_perf_extracted = data.apply(lambda x: x.artist_performing in x.video_title, axis=1).sum() / len(data)\n",
    "artist_orig_extracted = data.apply(lambda x: x.artist_original in x.video_title, axis=1).sum() / len(data)\n",
    "\n",
    "print(title_perf_extracted)\n",
    "print(title_orig_extracted)\n",
    "print(artist_perf_extracted)\n",
    "print(artist_orig_extracted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"\"\"\n",
    "You are a given a video title of a YouTube video which contains a musical performance. \n",
    "Parse the song title, the name of the performing artist in the video and the song title into the \n",
    "respective fields.\n",
    "If the song title is not english, also fill the field title_english with the english translation of the \n",
    "title you find. Here are some examples: \"\n",
    "\"\"\"\n",
    "\n",
    "examples = \"\"\"\n",
    "Example 1 video title: 'Dream Theater Metropolis Pt. 1 (Live At Luna Park DVD)'; Output attributes: artist_performing: Dream Theater, artist_original: Dream Theater, \n",
    "title_performing: Metropolis Pt. 1, title_original: Metropolis Pt. 1\n",
    "Example 2 video title: '(Måneskin) Beggin' - Fingerstyle Guitar Cover | Josephine Alexandra'; Output attributes: artist_performing: Josephine Alexandra, artist_original: Måneskin, \n",
    "title_performing: Beggin' - Fingerstyle Guitar Cover, title_original: Beggin'\n",
    "Example 3 video title: 'Owane - Rock Is Too Heavy'; Output attributes: artist_performing: Owane, artist_original: Owane, \n",
    "title_performing: Rock Is Too Heavy, title_original: Rock Is Too Heavy\n",
    "Example 4 video title: 'Jungle - Tash Sultana - Tutorial - Guitar Loop Cover - Tabs Available'; Output attributes: artist_performing: null, artist_original: Tash Sultana, \n",
    "title_performing: Jungle, title_original: Jungle\n",
    "Example 5 video title: 'How to Play Beat It Solo - Eddie Van Halen Michael Jackson'; Output attributes: artist_performing: null, artist_original: Micheal Jackson and Eddie Van Halen, \n",
    "title_performing: Beat It Solo, title_original: Beat It\n",
    "\n",
    "Input: \n",
    "\"\"\"\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"artist_performing\": {\"type\": \"string\"},\n",
    "        \"artist_original\": {\"type\": \"string\"},\n",
    "        \"title_original\": {\"type\": \"string\"},\n",
    "        \"title_performing\": {\"type\": \"string\"},\n",
    "        \"title_english\": {\"type\": \"string\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "outputs = []\n",
    "\n",
    "# few shot inference with Llama\n",
    "for video_title in tqdm(data.video_title): \n",
    "    prompt = role_description + task_description + examples + video_title\n",
    "    output = llama.prompt_to_json(prompt, json_schema, temperature)\n",
    "    outputs.append(output)\n",
    "\n",
    "# write to df\n",
    "data_parsed = pd.DataFrame(outputs)\n",
    "for col in data_parsed.columns:\n",
    "    data[col] = data_parsed[col].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_title_perf = sum((data.title.str.lower() == data.title_performing.str.lower())) / len(data)\n",
    "accuracy_title_orig = sum((data.title.str.lower() == data.title_original.str.lower())) / len(data)\n",
    "accuracy_artist_perf = sum((data.performer.str.lower() == data.artist_performing.str.lower())) / len(data)\n",
    "accuracy_artist_orig = sum((data.performer.str.lower() == data.artist_original.str.lower())) / len(data)\n",
    "\n",
    "print(accuracy_title_perf)\n",
    "print(accuracy_title_orig)\n",
    "print(accuracy_artist_perf)\n",
    "print(accuracy_artist_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_perf_extracted = data.apply(lambda x: x.title_performing in x.video_title, axis=1).sum() / len(data)\n",
    "title_orig_extracted = data.apply(lambda x: x.title_original in x.video_title, axis=1).sum() / len(data)\n",
    "artist_perf_extracted = data.apply(lambda x: x.artist_performing in x.video_title, axis=1).sum() / len(data)\n",
    "artist_orig_extracted = data.apply(lambda x: x.artist_original in x.video_title, axis=1).sum() / len(data)\n",
    "\n",
    "print(title_perf_extracted)\n",
    "print(title_orig_extracted)\n",
    "print(artist_perf_extracted)\n",
    "print(artist_orig_extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.performer != data.artist_performing][[\"video_title\", \"performer\", \"artist_performing\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "title_perf_ratios = data.apply(lambda x: fuzz.token_ratio(x.title_performing, x.video_title) / 100, axis=1)\n",
    "title_orig_ratios = data.apply(lambda x: fuzz.token_ratio(x.title_original, x.video_title) / 100, axis=1)\n",
    "artist_perf_ratios = data.apply(lambda x: fuzz.token_ratio(x.artist_performing, x.video_title)/ 100,  axis=1)\n",
    "artist_orig_ratios = data.apply(lambda x: fuzz.token_ratio(x.artist_original, x.video_title) / 100, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_perf_ratios.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_orig_ratios.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_perf_ratios.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_orig_ratios.describe()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
